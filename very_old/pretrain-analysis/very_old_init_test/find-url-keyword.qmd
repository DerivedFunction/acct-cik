---
title: "Fetch URL from CIK"
format: html
editor: source
jupyter: python3
---

# Initialization
## Installing required libraries

```{python}
# Installing required libraries
# %pip install pandas requests bs4
```

## Importing required libraries

```{python}
import pandas as pd  # Data processing
import requests      # Synchronous HTTP requests
import time          # For rate limiting
from bs4 import BeautifulSoup  # Webpage extraction
import json          # JSON parsing
import sqlite3       # To store our data
from typing import List
import random
import re # Regex expressions
```

## Initialization

```{python}
filename = "sample.csv"
DB_PATH = "../db/keyword_results.db"
EXCEL_PATH="../excel/keyword_results.xlsx"
KEYWORDS = [
    # General derivative disclosures
    "derivative liability",
    "derivative liabilities",
    "embedded derivative",
    "warrants",

    # Swaps
    "interest rate swap",
    "cross currency swap",
    "cross-currency swap",
    "total return swap",

    # Options
    "option contract",
    "options contract",

    # Forwards
    "forward contract",
    "foreign exchange contract",
    "forward foreign",
    "foreign currency future",

    # Other precise markers
    "notional amount",
    "not designated as hedging",
    "not designated as a hedging",
]
FILING_TYPES = {"10-K", "20-F"}
SEC_RATE_LIMIT = 5  # requests per second
df = pd.read_csv(filename)
df.head()
```

## Predefined Functions

## Debug printing
```{python}
debug = True # Debug printing
def debug_print(*args):
    if debug:
        print(*args)

debug_print("Debug Print status:", debug)
```

### Fetch URLs

```{python}
def rate_limit_sleep():
    """Simple sleep to respect SEC rate limits synchronously."""
    time.sleep(1 / SEC_RATE_LIMIT)


def fetch_json(url: str) -> dict | None:
    """Fetch a URL and return JSON as dict, or None if failed."""
    headers = {"User-Agent": f"testing {random.randint(1000,9999)}@example.com"}
    rate_limit_sleep()
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        print("Fetching", url)
        if resp.status_code == 429:
            retry_after = resp.headers.get("Retry-After")
            if retry_after:
                print(f"Rate limited. Retrying after {retry_after} seconds.")
                time.sleep(int(retry_after))
                return fetch_json(url)  # retry
        elif resp.status_code != 200:
            print(f"Error {resp.status_code} fetching {url}")
            return None
        return resp.json()
    except Exception as e:
        print(f"Exception fetching {url}: {e}")
        return None


def extract_filings(data: dict, cik: str, name: str, ticker: str) -> List[dict]:
    """Extract filings from SEC JSON data."""
    links = []
    forms = data.get("form", [])
    accession_numbers = data.get("accessionNumber", [])
    primary_docs = data.get("primaryDocument", [])
    filing_dates = data.get("filingDate", [])
    report_dates = data.get("reportDate", [])

    for i, f_type in enumerate(forms):
        if f_type in FILING_TYPES:
            accession = accession_numbers[i].replace("-", "")
            doc = primary_docs[i]
            if not doc or doc.endswith("txt"): # For old files
                doc = f"{accession[:10]}-{accession[10:12]}-{accession[12:]}.txt"
            link = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession}/{doc}"
            links.append({
                "name": name,
                "filing_date": filing_dates[i],
                "report_date": report_dates[i],
                "link": link,
                "ticker": ticker,
                "type": f_type
            })
    return links


def get_cik_filings(cik: str) -> List[dict]:
    """Fetch all filings for a CIK (synchronously)."""
    cik = str(cik).zfill(10)
    url_main = f"https://data.sec.gov/submissions/CIK{cik}.json"

    data = fetch_json(url_main)
    if not data:
        return []

    name = data.get("name", "")
    ticker = data.get("tickers", [])[0] if data.get("tickers", []) else cik

    # Recent filings
    recent = data.get("filings", {}).get("recent", {})
    links = extract_filings(recent, cik, name, ticker)

    # Older filings
    older_files = data.get("filings", {}).get("files", [])
    for f in older_files:
        older_data = fetch_json(f"https://data.sec.gov/submissions/{f.get('name')}")
        if isinstance(older_data, dict):
            links.extend(extract_filings(older_data, cik, name, ticker))

    return links
```

### Extract Content from Webpage

```{python}
def extract_content(html: str) -> dict:
    """
    Extract text from HTML

    Returns text as a string
    - 'text': str
    """
    if not html:
        return {"paragraphs": []}
    debug_print("Parsing Output")
    soup = BeautifulSoup(html, "html.parser")

    text = soup.get_text(separator='\n', strip=True)
    return text


def extract_text(text: str):
    """
    Extract paragraphs using simplified fixed rules.
    """
    
    # Strip double newlines and spaces
    cleaned_text = re.sub(r"\n\s*\n", "\n", text)

    return cleaned_text

```

### Match text

```{python}
def find_count_by_keywords(content: str, year: int) -> dict:
    """
    Find the count for each keyword.
    """
    keywords_lower = [kw.lower() for kw in KEYWORDS]
    keyword_counts = {kw: 0 for kw in keywords_lower}

    for keyword in keywords_lower:
        keyword_counts[keyword] = len(re.findall(r"\b" + re.escape(keyword) + r"\b", content.lower()))

    return keyword_counts
    
```

### Process Each URL

```{python}
def fetch_url(url: str, timeout: int = 10) -> str | None:
    """
    Fetch the content of a webpage synchronously.
    """
    if not url:
        return None
    try:
        rate_limit_sleep()
        print("Fetching", url)
        resp = requests.get(url, timeout=timeout, headers={"User-Agent": "sync-fetch@example.com"})
        if resp.status_code != 200:
            print(f"Error {resp.status_code} for {url}")
            return None
        return resp.text
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None


def process_url(url: str, year: int):
    """
    Processes a url and its data synchronously.
    """
    data = fetch_url(url)
    if not data:
        return {}
    content = None
    if url.endswith("htm"):
        debug_print("Processing as html")
        content = extract_content(data)
    else:
        debug_print("Processing as text")
        content = extract_text(data)
    filtered = find_count_by_keywords(content, year)
    return filtered
```

## Database Functions

```{python}
def create_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute("""
            CREATE TABLE IF NOT EXISTS webpage_results (
                cik INTEGER,
                year INTEGER,
                url TEXT PRIMARY KEY,
                json_data TEXT
            )
        """)
        c.execute("""
            CREATE TABLE IF NOT EXISTS fail_results (
                cik INTEGER,
                year INTEGER,
                url TEXT PRIMARY KEY
            )
        """)
    except sqlite3.IntegrityError:
        debug_print("Something went wrong creating the database")
    finally:
        conn.commit()
        conn.close()


def save_result(result, df):
    """
    Save a single page result to SQLite.
    If insert fails (e.g., duplicate url), insert into fail_results.
    """
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute(
            "INSERT INTO webpage_results (url, cik, year, json_data) VALUES (?, ?, ?, ?)",
            (df.link, df.cik, df.year, json.dumps(result))
        )
    except sqlite3.IntegrityError:
        # Insert into fail_results if duplicate or other integrity error
        c.execute(
            "INSERT OR IGNORE INTO fail_results (url, cik, year) VALUES (?, ?, ?)",
            (df.link, df.cik, df.year)
        )
    conn.commit()
    conn.close()

def fetch_webpage_results():
    """
    Fetch results from webpage_results
    """
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT * FROM webpage_results")
    columns = [col[0] for col in c.description]
    rows = c.fetchall()
    pre_data = pd.DataFrame(rows, columns=columns)
    conn.close()
    return pre_data

create_db()
```

## Merge it all together

```{python}
def filter_by_fyear(filings: list[dict], fyear: int) -> list[dict]:
    """Filter filings by fiscal year."""
    return [f for f in filings if f.get("report_date") and f.get("report_date").startswith(str(fyear))]

def fetch_all_grouped(df: pd.DataFrame) -> pd.DataFrame:
    """
    Fetch filings once per CIK, filter by fiscal years, 
    and return as a new DataFrame.
    """
    records = []

    # group: {cik -> [fyears]}
    cik_groups = df.groupby("cik")["fyear"].apply(list).reset_index()

    for row in cik_groups.itertuples(index=False):
        cik = row.cik
        years = row.fyear

        filings = get_cik_filings(cik)  # fetch once

        for fyear in years:
            year_filings = filter_by_fyear(filings, fyear)
            for filing in year_filings:
                records.append({
                    "cik": cik,
                    "year": fyear,
                    **filing  # expands filing dict into columns
                })

    return pd.DataFrame(records)
```

# Execute Main Loop
```{python}
report_results = fetch_all_grouped(df)
```

## Pre-Process each link and store in a database
```{python}
for report in report_results.itertuples(index=False):
    debug_print("Processing", report.cik, report.year)
    result = process_url(report.link, report.year)
    save_result(result, report)
```

```{python}
dp = fetch_webpage_results()
dp.head()
```

## Post-Process and store in database
### Functions
```{python}
def fetch_excel():
    return pd.read_excel(EXCEL_PATH)
def parse_json(data):
    try:
        return json.loads(data)
    except (json.JSONDecodeError, TypeError):
        return []
def plot_data(df):
    print(df.describe())
    plt.figure(figsize=(10,6))
    df.plot(kind='bar', stacked=True)
    plt.title("Distribution of Keyword Counts")
    plt.xlabel("Keywords")
    plt.ylabel("Count")
    plt.show()
```

### Begin
### 1. Parse the JSON string in 'json_data' into a Python list of keywords
We'll handle potential errors by returning an empty list if parsing fails
```{python}
dp = fetch_webpage_results()
dp['keywords'] = dp['json_data'].apply(parse_json)
dp.head()
```

### 2. "Apply" the DataFrame. 
Each keyword in the set gets its own column
```{python}
training_df = dp.json_normalize(dp['keywords'])
training_df.head()
```

### 3. Display the status of each keyword count.
```{python}
# We want to first merge similar keywords together
training_df["Derivative Liability"] = training_df["derivative liability"] + training_df["derivative liabilities"]

training_df["Cross-Currency Swap"] = training_df["cross currency swap"] + training_df["cross-currency swap"]

training_df["Options"] = training_df["option contract"] + training_df["options contract"]

training_df["Foreign Exchange Contract"] = training_df["forward contract"] + training_df["foreign exchange contract"] + training_df["forward foreign"] + training_df["foreign currency future"]

training_df["Not Designated As Hedging"] = training_df["not designated as hedging"] + training_df["not designated as a hedging"]

# Drop unneeded columns
training_df = training_df[['cik', 'year', 'Derivative Liability', 'warrants', 'Cross-Currency Swap', 'Options', 'Foreign Exchange Contract', 'Not Designated As Hedging', "notional amount", "total return swap"]]

# Plot
plot_data(training_df)
```

### Display the first few rows of the new, transformed DataFrame and save it for labeling
```{python}
print(training_df.head())
# Save the dataframe to an excel file
training_df.to_excel(EXCEL_PATH, index=False)