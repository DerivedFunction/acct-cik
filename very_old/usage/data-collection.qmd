---
title: "Get Data"
format: html
editor: source
jupyter: python3
---

# Importing required libraries

```{python}
# Installing required libraries
# %pip install pandas requests bs4 matplotlib tqdm
```

```{python}
import pandas as pd  # Data processing
import requests      # Synchronous HTTP requests
import time          # For rate limiting
from bs4 import BeautifulSoup  # Webpage extraction
import json          # JSON parsing
import sqlite3       # To store our data
from typing import List
import random
import re # Regex expressions
from concurrent.futures import ThreadPoolExecutor, as_completed # Concurrency
from tqdm import tqdm
```

```{python}
filename = "derivatives_data.csv"
DB_PATH = "../db/web_data.db"
KEYWORDS_FILE = "../config/keywords_label.json"
NUM_THREADS = 4
SEC_RATE_LIMIT = 1 / 5 # requests per second
```

# Regex Patterns and Keyword Matching
```{python}
IGNORE_KEYWORDS = {
    "table of contents",
    "consolidated",
    "balance sheet",
    "BEGIN PRIVACY-ENHANCED MESSAGE",
    "us-gaap:",
}
FILING_TYPES = {
    # Current
    "10-K",
    "10-KT",
    "20-F",
    "40-F",
    # Legacy
    "10-K405",
    "10KSB",
    "10KSB40",
}

# temporarily replace whitelisted currency symbols with placeholders
PLACEHOLDERS = {
    "€": "__EURO__",
    "£": "__POUND__",
    "¥": "__YEN__",
    "¢": "__CENTS__",
}
REPLACE_HOLDERS = PLACEHOLDERS | {
    "•": "*",
    "—": "--",
    "“": '"',
    "”": '"',
    "‘": "'",
    "’": "'",
}
# Compile regex patterns once
NON_ASCII_PATTERN = re.compile(r"[^\x00-\x7F]+")
BULLET_PATTERN = re.compile(r"^[-*•]\s*")
NUMBERED_PATTERN = re.compile(r"^\(?\d+[\.\)]\s+")
PUNCTUATION_END_PATTERN = re.compile(r"[.!?;:•)]$")
SENTENCE_SPLIT_PATTERN = re.compile(r"(?<![A-Z0-9])\s*\.\s*(?![a-zA-Z0-9])")
CRUNCHED_TEXT_PATTERNS = [
    (re.compile(r"([a-z])([A-Z])"), r"\1 \2"),
    (re.compile(r"([a-zA-Z])(\d+)"), r"\1 \2"),
    (re.compile(r"(\d+)([a-zA-Z])"), r"\1 \2"),
    (re.compile(r"([a-zA-Z0-9])(\$)"), r"\1 \2"),
]
CLEANUP_PATTERNS = [
    (re.compile(r"\s+"), " "),
    (re.compile(r"\(\s*"), "("),
    (re.compile(r"\s*\)"), ")"),
    (re.compile(r"\s*,"), ","),
    (re.compile(r"(-{3,}|={3,}|\.{3,})"), ""),
    (re.compile(r"<.*?>"), ""),
    (re.compile(r"table of contents", re.IGNORECASE), ""),
    (re.compile(r"F-\d+"), ""),
]
# Pre-compile patterns
SEPARATOR_PATTERN = re.compile(r"[-=\s]+")
CAPTION_PATTERN = re.compile(r"<CAPTION>", re.IGNORECASE)
COLUMN_SPLIT_PATTERN = re.compile(r"\s{2,}")
# Use more efficient split with compiled regex
TABLE_SPLIT_PATTERN = re.compile(
    r"(<TABLE>.*?</TABLE>)", re.DOTALL | re.IGNORECASE
)
```
```{python}
all_derivatives_df = pd.read_csv(filename)

with open(KEYWORDS_FILE, "r", encoding="utf-8") as f:
    keyword_data = json.load(f)

id2label = {int(k): v for k, v in keyword_data["id2label"].items()}

MATCHING_KEYWORDS = {int(k): v for k, v in keyword_data["MATCHING_KEYWORDS"].items()}
# Flatten all keywords, handling nested dicts for label 0
KEYWORDS = set()
for kws in MATCHING_KEYWORDS.values():
    if isinstance(kws, dict):  # nested types
        for sublist in kws.values():
            KEYWORDS.update(sublist)
    else:
        pass
```

## Predefined Functions

### Debug printing

```{python}
debug = False # Debug printing
def debug_print(*args):
    if debug:
        print(*args)
```

## Database Functions

```{python}
def create_db():
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    try:
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS report_data (
                cik INTEGER,
                year INTEGER,
                url TEXT
            )
        """
        )
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS names (
                cik INTEGER,
                name TEXT
            )
        """
        )
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS webpage_result (
                url TEXT,
                matches TEXT
            )
        """
        )
        c.execute(
            """
            CREATE TABLE IF NOT EXISTS fail_results (
                cik INTEGER,
                year INTEGER,
                url TEXT,
                reason TEXT
            )
        """
        )
        c.execute(
            """
            CREATE INDEX IF NOT EXISTS url_idx ON report_data (url)
            """
        )
        c.execute(
            """
            CREATE INDEX IF NOT EXISTS url_idx ON webpage_result (url)
            """
        )
        c.execute(
            """
            CREATE INDEX IF NOT EXISTS name_idx ON 
            names (name)
            """
        )
    except sqlite3.IntegrityError:
        print("Something went wrong creating the database")
    finally:
        conn.commit()
        conn.close()


def save_batch_report_urls(df):
    """
    Saves a dataframe of link to the database.
    """
    with sqlite3.connect(DB_PATH) as conn:
        try:
            name = df[["cik", "name"]].drop_duplicates()
            name = name.dropna()
            name["name"] = name["name"].str.title()
            name.to_sql("names", conn, if_exists="append", index=False)
        except:
            pass
        try:
            report = df[["cik", "year", "url"]]
            report.to_sql("report_data", conn, if_exists="append", index=False)
            return True
        except sqlite3.IntegrityError:
            debug_print(df.head())
            df = df[["cik", "year", "url"]]
            df["reason"] = "Error submitting batch"
            df.to_sql("fail_results", conn, if_exists="append", index=False)
            return False


def fetch_report_data(valid=True):
    """
    Fetch all report urls from the database.
    """
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    if valid:
        c.execute("SELECT * FROM report_data WHERE NOT url =''")
    else:
        c.execute("SELECT * FROM report_data WHERE url =''")
    columns = [col[0] for col in c.description]
    rows = c.fetchall()
    pre_data = pd.DataFrame(rows, columns=columns)
    conn.close()
    return pre_data


def fetch_webpage_results():
    """
    Fetch results from webpage_results
    """
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT * FROM webpage_result")
    columns = [col[0] for col in c.description]
    rows = c.fetchall()
    pre_data = pd.DataFrame(rows, columns=columns)
    conn.close()
    return pre_data


def get_processed_urls() -> set[tuple]:
    """
    Return a set of (url) that are already processed in `webpage_result`.
    """
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute("SELECT url FROM webpage_result")
    rows = c.fetchall()
    conn.close()
    return set(rows)


def save_process_result(df):
    """
    Inserts a new item into the webpage_result table
    """
    conn = sqlite3.connect(DB_PATH)
    c = conn.cursor()
    c.execute(
        "INSERT OR REPLACE INTO webpage_result (url, matches) VALUES (?, ?)",
        (
            df.url,
            json.dumps(df.matches),
        ),
    )
    conn.commit()
    conn.close()


create_db()
existing_report_df = fetch_report_data()
print(f"Found {len(existing_report_df)} reports in database")
print(f"Found {len(fetch_report_data(False))} invalid/failed reports in database")
```

### Fetch report URLS

```{python}
def fetch_json(url: str) -> dict | None:
    """Fetch a URL and return JSON as dict, or None if failed."""
    headers = {"User-Agent": f"{random.randint(1000,9999)} {random.randint(1000,9999)}@{random.randint(1000,9999)}.com"}
    time.sleep(SEC_RATE_LIMIT)
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        debug_print("Fetching", url)
        if resp.status_code != 200:
            print(f"Error {resp.status_code} fetching {url}")
            return None
        return resp.json()
    except Exception as e:
        print(f"Exception fetching {url}: {e}")
        return None


def extract_filings(data: dict, cik: str, name: str, ticker: str) -> List[dict]:
    """Extract filings from SEC JSON data."""
    links = []
    forms = data.get("form", [])
    accession_numbers = data.get("accessionNumber", [])
    primary_docs = data.get("primaryDocument", [])
    filing_dates = data.get("filingDate", [])
    report_dates = data.get("reportDate", [])

    for i, f_type in enumerate(forms):
        if f_type in FILING_TYPES:
            accession = accession_numbers[i].replace("-", "")
            doc = primary_docs[i]
            if not doc or doc.endswith("txt"):  # For old files
                doc = f"{accession[:10]}-{accession[10:12]}-{accession[12:]}.txt"
            link = f"https://www.sec.gov/Archives/edgar/data/{cik}/{accession}/{doc}"
            links.append(
                {
                    "name": name,
                    "filing_date": filing_dates[i],
                    "report_date": report_dates[i],
                    "url": link,
                    "ticker": ticker,
                    "type": f_type,
                }
            )
    return links


def get_cik_filings(cik: str) -> List[dict]:
    """Fetch all filings for a CIK (synchronously)."""
    cik = str(cik).zfill(10)
    url_main = f"https://data.sec.gov/submissions/CIK{cik}.json"

    data = fetch_json(url_main)
    if not data:
        return None

    name = data.get("name", "")
    ticker = data.get("tickers", [])[0] if data.get("tickers", []) else cik

    # Recent filings
    recent = data.get("filings", {}).get("recent", {})
    links = extract_filings(recent, cik, name, ticker)

    # Older filings
    older_files = data.get("filings", {}).get("files", [])
    for f in older_files:
        older_data = fetch_json(f"https://data.sec.gov/submissions/{f.get('name')}")
        if isinstance(older_data, dict):
            links.extend(extract_filings(older_data, cik, name, ticker))

    return links
```

### Extract Content from Webpage

```{python}
def extract_content(data: str, asHTML=True, max_len=2000) -> str:
    """
    Extract and clean text from HTML or plaintext in one pass.
    Optimized version with reduced regex compilation and string operations.
    """
    if not data:
        return ""

    if asHTML:
        soup = BeautifulSoup(data, "html.parser")
        text = soup.get_text(separator="\n\n", strip=True)
        text = keep_allowed_chars(text, True)
        # Split into candidate paragraphs
        paragraphs = [p.strip() for p in re.split(r"\n\s*\n", text) if p.strip()]
        merged_paragraphs = []

        i = 0
        while i < len(paragraphs):
            line = paragraphs[i]

            # Check for bullets
            if BULLET_PATTERN.match(line):
                # Merge single-character bullet with next line
                if len(line.strip()) == 1 and i + 1 < len(paragraphs):
                    line = f"{line} {paragraphs[i + 1]}"
                    i += 1  # Skip next line

                # Merge with previous bullet if applicable
                if merged_paragraphs and BULLET_PATTERN.match(merged_paragraphs[-1]):
                    merged_paragraphs[-1] += f"\n{line}"
                else:
                    merged_paragraphs.append(line)

            # Check for numbered lists
            elif NUMBERED_PATTERN.match(line):
                if merged_paragraphs and NUMBERED_PATTERN.match(merged_paragraphs[-1]):
                    merged_paragraphs[-1] += f"\n{line}"
                else:
                    merged_paragraphs.append(line)

            # Merge if previous line doesn't end in punctuation
            elif merged_paragraphs and not PUNCTUATION_END_PATTERN.search(
                merged_paragraphs[-1]
            ):
                merged_paragraphs[-1] += f" {line}"
            else:
                merged_paragraphs.append(line)

            i += 1

        # Handle max length with pre-compiled regex
        final_paragraphs = []
        for para in merged_paragraphs:
            if len(para) <= max_len:
                final_paragraphs.append(para)
            else:
                parts = SENTENCE_SPLIT_PATTERN.split(para)
                current_chunk = ""

                for part in parts:
                    if len(current_chunk) + len(part) + 1 <= max_len:
                        current_chunk += f" {part}" if current_chunk else part
                    else:
                        if current_chunk:
                            final_paragraphs.append(current_chunk)
                        current_chunk = part

                if current_chunk:
                    final_paragraphs.append(current_chunk)

        paragraphs = final_paragraphs

    else:  # Plain text processing
        text = keep_allowed_chars(data)
        parts = TABLE_SPLIT_PATTERN.split(text)
        paragraphs = []

        for part in parts:
            if part.strip().lower().startswith("<table>"):
                rows = parse_plain_text_table_fixed(part)
                table_text = "\n".join(["\t".join(row) for row in rows])
                paragraphs.append(table_text)
            else:
                # Use list comprehension for better performance
                sub_paras = [p for p in re.split(r"\n\s*\n", part) if p.strip()]
                paragraphs.extend(sub_paras)

    # Optimized cleaning loop
    cleaned_paragraphs = []
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        # Keep newlines for bullets and numbered lists (check once)
        is_list_item = BULLET_PATTERN.match(para) or NUMBERED_PATTERN.match(para)
        if not is_list_item:
            para = re.sub(r"\n+", " ", para)

        # Apply crunched text fixes
        for pattern, replacement in CRUNCHED_TEXT_PATTERNS:
            para = pattern.sub(replacement, para)

        # Apply cleanup patterns
        for pattern, replacement in CLEANUP_PATTERNS:
            para = pattern.sub(replacement, para)

        para = para.strip()

        # Merge short paragraphs
        if len(para) < 15 and cleaned_paragraphs:
            cleaned_paragraphs[-1] = f"{cleaned_paragraphs[-1]} {para}"
        elif para:
            cleaned_paragraphs.append(para)

    return "\n\n".join(cleaned_paragraphs)


def keep_allowed_chars(text, asHTML=False):
    """Optimized version with single pass processing"""
    if not isinstance(text, str):
        return text  # Ignore non-strings

    # Try HTML decode if requested
    if asHTML:
        try:
            text = text.encode("utf-8").decode("unicode_escape")
        except Exception:
            pass  # Leave text as-is if decoding fails

    # Apply placeholders
    for sym, ph in REPLACE_HOLDERS.items():
        text = text.replace(sym, ph)

    # Remove non-ASCII in single pass
    text = NON_ASCII_PATTERN.sub("", text)

    # Restore placeholders
    for sym, ph in PLACEHOLDERS.items():
        text = text.replace(ph, sym)
    return text


def parse_plain_text_table_fixed(block: str):
    """
    Optimized table parsing with pre-compiled patterns
    """
  

    rows = []
    first_col_buffer = ""
    first_col_active = True

    # Process lines more efficiently
    lines = [line.rstrip() for line in block.splitlines() if line.strip()]

    for line in lines:
        if SEPARATOR_PATTERN.fullmatch(line) or CAPTION_PATTERN.match(line.strip()):
            continue

        if "<S>" in line:
            first_col_active = False
            line = line.replace("<S>", "").lstrip()

        if first_col_active:
            first_col_buffer = f"{first_col_buffer} {line.strip()}".strip()
            continue

        # Split columns efficiently
        cols = [col.strip() for col in COLUMN_SPLIT_PATTERN.split(line)]

        if first_col_buffer:
            rows.append([first_col_buffer] + cols)
            first_col_buffer = ""
        else:
            rows.append(cols)

    return rows


def fetch_url(url: str, timeout: int = 10) -> str | None:
    """
    Fetch the content of a webpage synchronously.
    """
    if not url:
        return None
    try:
        time.sleep(SEC_RATE_LIMIT)
        debug_print("Fetching", url)
        resp = requests.get(
            url, timeout=timeout, headers={"User-Agent": "sync-fetch@example.com"}
        )
        if resp.status_code != 200:
            print(f"Error {resp.status_code} for {url}")
            return None
        return resp.text
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None


def process_url(url: str, cik: int, year: int):
    """
    Processes a URL and its data synchronously.
    Saves the extracted content in the DB if not already cached.
    """
    debug_str = f"Processing {cik} for year {year}"
     raw_text = fetch_url(url)
    if not raw_text:
        return ""
    # Extract content
    if url.endswith("htm"):
        debug_print("Processing as html")
        content = extract_content(raw_text, True)
    else:
        debug_print("Processing as text")
        content = extract_content(raw_text, False)
```

### Keywords Finder: Find keywords

```{python}
def find_count_by_keywords(content: str, year: int) -> dict:
    """
    Find the count for each keyword.
    """
    keywords_lower = [kw.lower() for kw in KEYWORDS]
    keyword_counts = {kw: 0 for kw in keywords_lower}

    for keyword in keywords_lower:
        keyword_counts[keyword] = len(re.findall(r"\b" + re.escape(keyword) + r"\b", content.lower()))

    return keyword_counts
```

### Match Text: Find matching keywords and extract the paragraphs

```{python}

def filter_by_keywords(content: str, year: int, min_char_length: int = 400, max_char_length=2000) -> dict:
    """
    Extract and filter sentences based on keywords with simple context expansion.
    Optimized for speed with pre-cleaning and minimal keyword searches.
    """
    keywords_lower = [kw.lower() for kw in KEYWORDS]
    ignore_keywords = [kw.lower() for kw in IGNORE_KEYWORDS]
    debug_print("Generating sentences")
    def matches_text(text: str) -> bool:
        return bool(text) and any(kw in text.lower() for kw in keywords_lower)
    
    def clean_sentence(sentence: str) -> str:
        """Apply consistent cleaning to a single sentence"""
        return re.sub(r"\s+", " ", sentence.strip())
    
    def measure_merged_length(sentences: list) -> int:
        """Calculate length of merged sentences with proper formatting"""
        return len(". ".join(sentences).strip() + ".")
    
    def should_ignore(text: str) -> bool:
        """Check if text should be ignored based on keywords"""
        normalized = text.lower()
        return any(kw in normalized for kw in ignore_keywords)
    
    def expand_context(all_sentences: list, target_idx: int, has_keyword: bool) -> str:
        """Expand context around target sentence with randomized min_char_length and truncation"""
        # Randomize min length within range (e.g., 600–800)
        random.seed(year + target_idx)  # stable per target sentence
        dynamic_min = random.randint(int(min_char_length * 0.75), int(min_char_length * 1.25))

        merged = [all_sentences[target_idx]]
        left_idx = target_idx - 1
        right_idx = target_idx + 1
        
        while True:
            current_length = measure_merged_length(merged)
            if current_length >= dynamic_min:
                break

            added = False

            # Try adding left
            if left_idx >= 0:
                candidate = [all_sentences[left_idx]] + merged
                if measure_merged_length(candidate) <= max_char_length:
                    merged.insert(0, all_sentences[left_idx])
                    left_idx -= 1
                    added = True

            # Try adding right
            if right_idx < len(all_sentences):
                candidate = merged + [all_sentences[right_idx]]
                if measure_merged_length(candidate) <= max_char_length:
                    merged.append(all_sentences[right_idx])
                    right_idx += 1
                    added = True

            if not added:
                break

            # Limit expansion for non-keyword sentences
            if not has_keyword and len(merged) >= 2:
                break

        # --- enforce max_char_length by truncating ---
        final_text = ". ".join(merged).strip() + "."
        if len(final_text) > max_char_length:
            excess = len(final_text) - max_char_length
            trim_left = excess // 2
            trim_right = excess - trim_left
            final_text = final_text[trim_left:len(final_text)-trim_right].strip()

        return final_text

    # Pre-process and clean all sentences from entire content
    text = re.sub(r"\s+", " ", content.strip())
    raw_sentences = [s.strip() for s in re.split(r'(?<![A-Z0-9])\s*\.\s*(?![a-zA-Z0-9])', text) if s.strip()]
    
    # Clean sentences once during preprocessing
    all_sentences = [clean_sentence(sentence) for sentence in raw_sentences]

    # Process sentences
    match_sentences = []
    seen_matches = set()    
    random.seed(year)

    for i, sentence in enumerate(all_sentences):
        if len(sentence.split()) < 6:
            continue

        # Quick keyword check first (most efficient)
        has_keyword = matches_text(sentence)
        
        # Only do expensive context expansion if needed
        if has_keyword:
            # Expand context around sentence
            final_sentence = expand_context(all_sentences, i, has_keyword)
            normalized = final_sentence.lower().strip()
            
            if should_ignore(normalized):
                continue
                
            if has_keyword:
                if normalized not in seen_matches:
                    seen_matches.add(normalized)
                    match_sentences.append(final_sentence)
        
    debug_print("Done generating sentences", len(match_sentences))
    return {
        "matches": match_sentences,
    }

```

## Merge it all together

```{python}
def filter_by_fyear(filings: list[dict], fyear: int) -> list[dict]:
    """Filter filings by fiscal year."""
    return [
        f
        for f in filings
        if f.get("report_date") and f.get("report_date").startswith(str(fyear))
    ]


def fetch_all_grouped(saveIteration: int = 10) -> pd.DataFrame:
    global existing_report_df, all_derivatives_df
    """
    Fetch filings once per CIK, filter by fiscal years,
    and return as a new DataFrame.
    Skips already-processed (cik, year) pairs if they exist in existing_report_df.
    Saves intermediate results every `saveIteration` CIKs.
    Uses ThreadPoolExecutor to process CIKs concurrently.
    """
    records = []

    if existing_report_df is None or existing_report_df.empty:
        existing_report_df = pd.DataFrame(columns=["cik", "year"])

    already_done = set(zip(existing_report_df["cik"], existing_report_df["year"]))
    # group: {cik -> [fyears]}
    cik_groups = all_derivatives_df.groupby("cik")["year"].apply(list).reset_index()

    def process_cik(row):
        """Process a single CIK and return records for all its years."""
        cik = row.cik
        years = row.year
        cik_records = []

        # Only fetch if at least one year is missing
        years_to_fetch = [y for y in years if (cik, y) not in already_done]
        if not years_to_fetch:
            return cik_records

        debug_print("Fetching", years_to_fetch)
        filings = get_cik_filings(cik)  # fetch once
        if filings is None:
            print("Error fetching filings for", cik)
            return cik_records
        for fyear in years_to_fetch:
            year_filings = filter_by_fyear(filings, fyear)
            for filing in year_filings:
                cik_records.append({"cik": cik, "year": fyear, **filing})

        # If we don't have the fyears, we use dummy urls
        for year in years:
            if (cik, year) not in already_done:
                cik_records.append({"cik": cik, "year": year, "url": ""})

        return cik_records

    # Process CIKs concurrently
    total_ciks = len(cik_groups)
    SEC_RATE_LIMIT = NUM_THREADS / 5  # requests per second
    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        # Submit all tasks
        future_to_cik = {
            executor.submit(process_cik, row): i
            for i, row in enumerate(cik_groups.itertuples(index=False), start=1)
        }

        # Process completed futures with progress bar
        for future in tqdm(as_completed(future_to_cik), total=len(future_to_cik)):
            i = future_to_cik[future]
            try:
                cik_records = future.result()
                records.extend(cik_records)

                # Periodic save
                if i % saveIteration == 0 and records:
                    new_df = save_batch_report_urls(pd.DataFrame(records))
                    debug_print(f"Saved {len(records)} urls to database")
                    records = []
            except Exception as exc:
                print(f"CIK processing generated an exception: {exc}")
    # Final save
    if records:
        new_df = save_batch_report_urls(pd.DataFrame(records))
        print(f"Saved {len(records)} urls to database")

    return fetch_report_data()


def process_all_reports_fully():
    processed_set = get_processed_urls()

    def process_keywords_for_report(report):
        url = report.url
        if (url,) in processed_set:  # Fix: Check for tuple
            debug_print(f"Skipping already processed: {url}")
            return None

        try:
            content = process_url(report.url, report.cik, report.year)
            if not content:
                debug_print(f"Failed to process {url}: No content.")
                return None

            sentences = filter_by_keywords(content, report.year)

            result_row = pd.Series(
                {
                    "url": url,
                    "matches": sentences["matches"],
                    "name": report.name,
                }
            )
            debug_print(f"Processed {result_row}")
            save_process_result(result_row)

            # Update the set to prevent race conditions
            processed_set.add((url,))
            return True
        except Exception as e:
            print(f"Error processing {url}: {e}")
            return None

    # Only process reports not already in webpage_result
    reports_to_process = [
        r
        for r in existing_report_df.itertuples(index=False)
        if (r.url,) not in processed_set and r.url
    ]
    print(
        f"Processing {len(reports_to_process)} new reports, {len(processed_set)} reports already processed"
    )

    results = 0
    empty_results = 0
    SEC_RATE_LIMIT = NUM_THREADS / 5  # requests per second
    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:
        future_to_report = {
            executor.submit(process_keywords_for_report, r): r
            for r in reports_to_process
        }

        for future in tqdm(as_completed(future_to_report), total=len(future_to_report)):
            res = future.result()
            if res is not None:
                results += 1
            else:
                empty_results += 1

    print(
        f"Processed {results + empty_results} new reports in parallel (Empty: {empty_results}, Processed: {results})"
    )
```

# Execute Main Loop

## Fetch from the csv all the 10-K report URLS

```{python}
# Click run above to init
# Returns all the 10-K links
fetch_all_grouped()
```


## Parallel execution: Perform keyword extraction

And then save the results to the database

```{python}
process_all_reports_fully()
```